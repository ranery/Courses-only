%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt]{article} % Default font size is 12pt
%\documentclass[14pt]{extarticle}

\usepackage{geometry}
\geometry{left = 2cm, right = 2cm, top = 2.0cm, bottom = 2.5cm}

\usepackage{graphicx}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{lipsum}

\usepackage{amsmath}
\numberwithin{equation}{section}

\usepackage{indentfirst}
\setlength{\parindent}{2em}

\usepackage{cite}

% \usepackage{times}
% \usepackage{mathpazo}
% \usepackage[garamond]{mathdesign}
\usepackage{charter}

\usepackage[colorlinks,linkcolor=black,urlcolor=blue]{hyperref}

\linespread{1}

\def\cents{\hbox{\rm\rlap/c}}

\graphicspath{{Pictures/}}

\usepackage{listings}
\usepackage[framed,numbered,autolinebreaks,useliterate]{mcode}
\usepackage{xcolor}
\lstset{
    columns=fixed,
    numbers=left,                                        % 在左侧显示行号
    keywordstyle=\color[RGB]{40,40,255},                 % 设定关键字颜色
    numberstyle=\footnotesize\color{darkgray},           % 设定行号格式
    commentstyle=\it\color[RGB]{0,96,96},                % 设置代码注释的格式
    stringstyle=\rmfamily\slshape\color[RGB]{128,0,0},   % 设置字符串格式
    showstringspaces=false,                              % 不显示字符串中的空格
    language=matlab,                                     % 设置语言
}

\usepackage[linewidth=1pt]{mdframed}

%----------------------------------------------------------------------------------------
\renewcommand{\baselinestretch}{1.2}        % 定义行距
%----------------------------------------------------------------------------------------
\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\center

\textsc{\LARGE Huazhong University of Science and Technology}\\[1.5cm]
\textsc{\Large Advanced Class 1501}\\[0.5cm]
\textsc{\Large Projects for Information Theory}\\[1.2cm]

\HRule \\[0.4cm]
{ \huge \bfseries Implementation of Huffman Coding and Shannon Coding}\\[0.4cm]
\HRule \\[1.5cm]

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
Haoran \textsc{You} % Your name
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisor:} \\
Yayu \textsc{Gao} % Supervisor's Name
\end{flushright}
\end{minipage}\\[4cm]

{\large \today}\\[3cm]


\vfill

\end{titlepage}

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

\tableofcontents

\newpage

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\section{Introduction} % Major section

Here we will talk with two Coding Methods: \emph{Huffman Coding} and \emph{Shannon Coding} for the document \emph{Steve Jobs Speech.doc}. It was presented by \emph{Shannon}\footnote{who published $\ll$A Mathematical Theory of Communication$\gg$ in Bell System Technical Journal.1948.} the first time that the concept of information entropy and the \emph{Shannon's First theorem}, which is called \emph{zero-error coding theorem} for foreseeing the average code length per symbol of an instantaneous code can be made as closes to the entropy, but never be smaller.

%----------------------------------------------------------------------------------------
%	MAJOR SECTION 1
%----------------------------------------------------------------------------------------

\section{Restatement of the Problem}
The objective of this programming project is to deeply understand the Huffman coding and Shannon coding method. Here are the assignment:
\begin{itemize}
  \item Collect the statistics of the letters, punctuation, space in the document \emph{"Steve Jobs Speech.doc"}.
  \item Compute the entropy of \emph{"Steve Jobs Speech.doc"}.
  \item Apply the \emph{Huffman coding method} and \emph{Shannon coding method} for \emph{“Steve Jobs Speech.doc”}. Output the letters/punctuation/space and their Huffman codewords and Shannon codes.
respectively.
  \item Decode the code which generated by the \emph{Huffman coding method} and \emph{Shannon coding method}, respectively.
  \item Compute the average code length of \emph{“Steve Jobs Speech.doc”} using Huffman codes and Shannon codes, respectively.
\end{itemize}


\section{Detailed requirements}

\subsection{Collect the statistics of \emph{Steve Jobs Speech.doc}}
First we should get the content of this document, the corresponding module in my code is \emph{"Load Data"}. And then we can vote for different symbols in this document to get the frequency of occurrence for each symbol, which corresponding to the module \emph{"Text Statistics"} in my code. The result can be seen from Table1: the number of different characteristics are $71- symbols$, and the number of total characteristics in this document are $11838-symbols$.


\begin{center}
  Table1: Symbols statistics in \emph{Steve Jobs Speech.doc}.
\end{center}

\setlength{\arrayrulewidth}{0.7mm}

\begin{table}[!hptb]
\begin{minipage}{.5\linewidth}
\centering
\begin{tabular}{ccc}  % {cccc} 表示各列元素对齐方式，left-l,right-r,center-c
\hline
symbols  &Times  &Probability \\ \hline  % \hline 在此行下面画一横线
         &28     &2.365264e-03\\                % \\ 表示重新开始一行
         &2228   &1.882075e-01\\                % & 表示列的分隔线
"        &12     &1.013685e-03\\
\$       &1      &8.447373e-05\\
'        &40     &3.378949e-03\\
,        &101    &8.531847e-03\\
-        &9      &7.602636e-04\\
.        &142    &1.199527e-02\\
0        &11     &9.292110e-04\\
1        &7      &5.913161e-04\\
2        &2      &1.689475e-04\\
3 		 &6      &5.068424e-04\\
4 		 &1      &8.447373e-05\\
5 		 &2      &1.689475e-04\\
6 		 &2      &1.689475e-04\\
7 		 &5      &4.223686e-04\\
8 		 &1      &8.447373e-05\\
9 		 &2      &1.689475e-04\\
: 		 &9      &7.602636e-04\\
; 		 &2      &1.689475e-04\\
? 		 &4      &3.378949e-04\\
A 		 &31     &2.618686e-03\\
B 		 &11     &9.292110e-04\\
C 		 &4      &3.378949e-04\\
D 		 &9      &7.602636e-04\\
E 		 &5      &4.223686e-04\\
F 		 &3      &2.534212e-04\\
G 		 &2      &1.689475e-04\\
H 		 &6      &5.068424e-04\\
I 		 &116    &9.798953e-03\\
J 		 &1      &8.447373e-05\\
K 		 &2      &1.689475e-04\\
L 		 &5      &4.223686e-04\\
M 		 &11     &9.292110e-04\\
N 		 &9      &7.602636e-04\\
O 		 &4      &3.378949e-04\\  \hline
\end{tabular}
\end{minipage}\begin{minipage}{.5\linewidth}
\centering
\begin{tabular}{ccc}  % {cccc} 表示各列元素对齐方式，left-l,right-r,center-c
\hline
symbols  &Times  &Probability \\ \hline
P 		 &4      &3.378949e-04\\
R 		 &5      &4.223686e-04\\
S 		 &22     &1.858422e-03\\
T 		 &22     &1.858422e-03\\
W 		 &12     &1.013685e-03\\
X 		 &3      &2.534212e-04\\
Y 		 &5      &4.223686e-04\\
a 		 &741    &6.259503e-02\\
b 		 &121    &1.022132e-02\\
c 		 &215    &1.816185e-02\\
d 		 &408    &3.446528e-02\\
e 		 &1074   &9.072478e-02\\
f 		 &203    &1.714817e-02\\
g 		 &205    &1.731711e-02\\
h 		 &434    &3.666160e-02\\
i 		 &526    &4.443318e-02\\
j 		 &8      &6.757898e-04\\
k 		 &63     &5.321845e-03\\
l 		 &397    &3.353607e-02\\
m 		 &205    &1.731711e-02\\
n 		 &588    &4.967055e-02\\
o 		 &768    &6.487582e-02\\
p 		 &179    &1.512080e-02\\
q 		 &4      &3.378949e-04\\
r 		 &493    &4.164555e-02\\
s 		 &489    &4.130765e-02\\
t 		 &906    &7.653320e-02\\
u 		 &283    &2.390607e-02\\
v 		 &115    &9.714479e-03\\
w 		 &233    &1.968238e-02\\
x 		 &13     &1.098158e-03\\
y 		 &252    &2.128738e-02\\
z 		 &4      &3.378949e-04\\
$-$  	 &3      &2.534212e-04\\
\cents   &1      &8.447373e-05\\  \hline
71       &11838  & 1 \\   \hline
\end{tabular}
\end{minipage}
\end{table}


\subsection{Compute the entropy of document}
The definition formula of Information entropy can be drived as:
\begin{equation}\label{ep:eqs}
  H(X) = \sum_{i=1}^{n} p_{i}\log \frac{1}{p_{i}}
\end{equation}
Where $X$ is a random variable, ${p_{i}}$ is its probability distribution.
If we view the document as a random variable, we can get it's probability distribution as Table1. So the entropy of this document can be calculated as:
\begin{equation}\label{ep:eqs}
    H_{Steve-Jobs-Speech} \approx 4.387752
\end{equation}

\vspace{3\baselineskip}

\subsection{Coding for the document}

\subsubsection{Huffman Coding}

The fundamental thought of \emph{Huffman Coding} is make the code symbols whose occurrence probability is large as short as possible. So we should sort the symbols according to its occurrence probability and link the two least likely outcome symbols as a new symbol whose occurrence probability equals to the sum of that probabilities of two original symbols.

Recur the symbols until we get two symbols only, so it's easy for us to code them as $'0'$ and $'1'$. Then we find their l-child and r-child, coding them as $'x0'$ and $'x1'$, respectively. Finally we can get the instantaneous code which has the average code length near to information entropy of this document, called \emph{Huffman code}.

\vspace{1\baselineskip}
Here is our implementation of \emph{Huffman Coding}:
\newpage
\begin{lstlisting}[language=matlab]
function w = Huffman(r, P, S)
%
% Function : Huffman coding
% input    : r --- the number of the source symbols
%            P --- the probability distribution of source symbols
%            S --- the number of source symbols si
% output   : w --- the Huffman codewords wi corresponding to si
%
format long;
if(r == 2)
    w{S(1)} = '0';
    w{S(2)} = '1';
else
    [P_descend, idx] = sort(P, 'descend');    % sort {Pi} in descending order
    S = S(idx);                               % sort {Si}
    P = [P_descend(1:r-2)  (P_descend(r-1) + P_descend(r))];    % update P
    lchild = S(r - 1);                        % left child
    rchild = S(r);                            % right child
    S = S(1:r-1);                             % update S
    w = Huffman(r-1, P, S);                   % recursion
    w{rchild} = [w{lchild} '1'];
    w{lchild} = [w{lchild} '0'];
end
\end{lstlisting}
\begin{center}
  Code1: Huffman Coding
\end{center}

The result of \emph{Huffman Coding} can be seen as:

\begin{center}
  Table2: Huffman code for \emph{Steve Jobs Speech.doc}
\end{center}

\setlength{\arrayrulewidth}{0.7mm}
\begin{table}[hptb]
\begin{minipage}{.5\linewidth}
\centering
\begin{tabular}{lcl}  % {cccc} 表示各列元素对齐方式，left-l,right-r,center-c
\hline
Symbols  &Pi 				 &Code\\   \hline
 		 &2.365264e-03 		 &110011010\\
  		 &1.882075e-01 		 &01\\
" 		 &1.013685e-03 		 &1100110111\\
\$ 		 &8.447373e-05 		 &0001000001111\\
' 		 &3.378949e-03 		 &00010011\\
, 		 &8.531847e-03 		 &1100111\\
- 		 &7.602636e-04 		 &10010100010\\
. 		 &1.199527e-02 		 &000101\\
0 		 &9.292110e-04 		 &0001001000\\  \hline
\end{tabular}
\end{minipage}\begin{minipage}{.5\linewidth}
\centering
\begin{tabular}{lcl}  % {cccc} 表示各列元素对齐方式，left-l,right-r,center-c
\hline
Symbols  &Pi 				 &Code\\   \hline
1 		 &5.913161e-04 		 &11001100110\\
2 		 &1.689475e-04 		 &1100110010110\\
3 		 &5.068424e-04 		 &11001100111\\
4 		 &8.447373e-05 		 &00010000011100\\
5 		 &1.689475e-04 		 &1100110010111\\
6 		 &1.689475e-04 		 &1100110010100\\
7 		 &4.223686e-04 		 &00010001111\\
8 		 &8.447373e-05 		 &00010000011101\\
9 		 &1.689475e-04 		 &1100110010101\\  \hline
\end{tabular}
\end{minipage}
\end{table}

\begin{table}[hptb]
\begin{minipage}{.5\linewidth}
\centering
\begin{tabular}{lcl}  % {cccc} 表示各列元素对齐方式，left-l,right-r,center-c
\hline
Symbols  &Pi 				 &Code\\   \hline
: 		 &7.602636e-04 		 &10010100011\\
; 		 &1.689475e-04 		 &1001010011010\\
? 		 &3.378949e-04 		 &100101001010\\
A 		 &2.618686e-03 		 &110011000\\
B 		 &9.292110e-04 		 &0001001001\\
C 		 &3.378949e-04 		 &100101001011\\
D 		 &7.602636e-04 		 &10010100000\\
E 		 &4.223686e-04 		 &00010010110\\
F 		 &2.534212e-04 		 &000100011100\\
G 		 &1.689475e-04 		 &1001010011011\\
H 		 &5.068424e-04 		 &00010000010\\
I 		 &9.798953e-03 		 &1010110\\
J 		 &8.447373e-05 		 &10010100110010\\
K 		 &1.689475e-04 		 &1001010011000\\
L 		 &4.223686e-04 		 &00010010111\\
M 		 &9.292110e-04 		 &0001000110\\
N 		 &7.602636e-04 		 &10010100001\\
O 		 &3.378949e-04 		 &100101001000\\
P 		 &3.378949e-04 		 &100101001001\\
R 		 &4.223686e-04 		 &00010010100\\
S 		 &1.858422e-03 		 &000100001\\
T 		 &1.858422e-03 		 &000100010\\
W 		 &1.013685e-03 		 &0001000000\\
X 		 &2.534212e-04 		 &000100011101\\
Y 		 &4.223686e-04 		 &00010010101\\
a 		 &6.259503e-02 		 &0000\\
b 		 &1.022132e-02 		 &1001011\\ \hline
\end{tabular}
\end{minipage}\begin{minipage}{.5\linewidth}
\centering
\begin{tabular}{lcl}  % {cccc} 表示各列元素对齐方式，left-l,right-r,center-c
\hline
Symbols  &Pi 				 &Code\\   \hline
c 		 &1.816185e-02 		 &110010\\
d 		 &3.446528e-02 		 &11011\\
e 		 &9.072478e-02 		 &1000\\
f 		 &1.714817e-02 		 &111010\\
g 		 &1.731711e-02 		 &110100\\
h 		 &3.666160e-02 		 &11000\\
i 		 &4.443318e-02 		 &0011\\
j 		 &6.757898e-04 		 &11001100100\\
k 		 &5.321845e-03 		 &10010101\\
l 		 &3.353607e-02 		 &11100\\
m 		 &1.731711e-02 		 &110101\\
n 		 &4.967055e-02 		 &0010\\
o 		 &6.487582e-02 		 &1111\\
p 		 &1.512080e-02 		 &111011\\
q 		 &3.378949e-04 		 &100101001110\\
r 		 &4.164555e-02 		 &10011\\
s 		 &4.130765e-02 		 &10100\\
t 		 &7.653320e-02 		 &1011\\
u 		 &2.390607e-02 		 &00011\\
v 		 &9.714479e-03 		 &1010111\\
w 		 &1.968238e-02 		 &101010\\
x 		 &1.098158e-03 		 &1100110110\\
y 		 &2.128738e-02 		 &100100\\
z 		 &3.378949e-04 		 &100101001111\\
$-$      &2.534212e-04 		 &000100000110\\
\cents 	 &8.447373e-05 		 &10010100110011\\
\\ \hline
\end{tabular}
\end{minipage}
\end{table}


\subsubsection{Shannon Coding}

The construction of \emph{Shannon Code} is very ingenious, it use the conversion of decimal and binary to get instantaneous coding method.

The basic steps of our Shannon Coding Algorithm are:

\begin{mdframed}[leftmargin=-10pt,rightmargin=-10pt]
\begin{description} % Numbered list example

\item[Step 1] Sort probability distribution ${P(i)}$ in descending order.

\item[Step 2] For i, changed from $1$ to $r$, where $r$ is total number of our symbols, we can calculate the cumulative distribution $F(i)$, and change it to binary description.

\item[Step 3] Then we should calculate the shortest code length for the probability $P(i)$, denoted as ${l(i)}$.

\item[Step 4] Take $l(i)$ digits after the dot of binary description $F(i)$ as the codeword for the source symbol $S(i)$.
\end{description}
\end{mdframed}

Here is our implementation of \emph{Shannon Coding}:
\begin{lstlisting}[language=matlab]
function w = Shannon(r, P, S)
%
% Function : Shannon coding
% input    : r --- the number of the source symbols
%            P --- the probability distribution of source symbols
%            S --- the number of source symbols si
% output   : w --- the Huffman codewords wi corresponding to si
%
format long;
[P_descend, idx] = sort(P, 'descend');      % sort {Pi} in descending order
F = zeros(1,r);                             % initialize {Fi}
l = zeros(1,r);                             % initialize {li}
for i = 1:r
    F(i) = sum(P_descend(1:i-1));           % cumulative distribution function
    l(i) = ceil(log2(1 / P_descend(i)));    % codelength
    F_binary = dec_to_bin(F(i), l(i));      % convert decimal to binary

    % take li digit after the dot as the codeword for the source symbol idx(i)
    w{idx(i)} = num2str(F_binary);          % convert to string
    w{idx(i)} = strrep(w{idx(i)}, ' ','');  % eliminate space
end
\end{lstlisting}
\begin{center}
  Code2: Shannon Coding
\end{center}

The result of \emph{Shannon Coding} can be seen as:

\begin{center}
  Table2: Shannon code for \emph{Steve Jobs Speech.doc}
\end{center}

\setlength{\arrayrulewidth}{0.7mm}

\begin{table}[!hptb]
\begin{minipage}{.5\linewidth}
\centering
\begin{tabular}{lcl}  % {cccc} 表示各列元素对齐方式，left-l,right-r,center-c
\hline
Symbols  &Pi 				 &Code \\     \hline
 		 &2.365264e-03 		 &111110011\\
  		 &1.882075e-01 		 &000\\
" 		 &1.013685e-03 		 &1111101110\\
\$ 		 &8.447373e-05 		 &11111111111001\\
' 		 &3.378949e-03 		 &111110000\\
, 		 &8.531847e-03 		 &1111010\\
- 		 &7.602636e-04 		 &11111100111\\
. 		 &1.199527e-02 		 &1110101\\
0 		 &9.292110e-04 		 &11111100010\\
1 		 &5.913161e-04 		 &11111101111\\
2 		 &1.689475e-04 		 &1111111110010\\
3 		 &5.068424e-04 		 &11111110000\\
4 		 &8.447373e-05 		 &11111111111010\\
5 		 &1.689475e-04 		 &1111111110100\\
6 		 &1.689475e-04 		 &1111111110101\\
7 		 &4.223686e-04 		 &111111100101\\
8 		 &8.447373e-05 		 &11111111111011\\
9 		 &1.689475e-04 		 &1111111110111\\
: 		 &7.602636e-04 		 &11111101001\\
; 		 &1.689475e-04 		 &1111111111000\\
? 		 &3.378949e-04 		 &111111101110\\
A 		 &2.618686e-03 		 &111110010\\
B 		 &9.292110e-04 		 &11111100011\\
C 		 &3.378949e-04 		 &111111101111\\
D 		 &7.602636e-04 		 &11111101010\\
E 		 &4.223686e-04 		 &111111100111\\
F 		 &2.534212e-04 		 &111111110110\\
G 		 &1.689475e-04 		 &1111111111001\\
H 		 &5.068424e-04 		 &11111110001\\
I 		 &9.798953e-03 		 &1110111\\
J 		 &8.447373e-05 		 &11111111111101\\
K 		 &1.689475e-04 		 &1111111111011\\
L 		 &4.223686e-04 		 &111111101000\\
M 		 &9.292110e-04 		 &11111100101\\
N 		 &7.602636e-04 		 &11111101100\\
O 		 &3.378949e-04 		 &111111110000\\ \hline
\end{tabular}
\end{minipage}\begin{minipage}{.5\linewidth}
\centering
\begin{tabular}{lcl}
\hline
Symbols  &Pi 				 &Code \\     \hline
P 		 &3.378949e-04 		 &111111110010\\
R 		 &4.223686e-04 		 &111111101010\\
S 		 &1.858422e-03 		 &1111101010\\
T 		 &1.858422e-03 		 &1111101011\\
W 		 &1.013685e-03 		 &1111101111\\
X 		 &2.534212e-04 		 &111111110111\\
Y 		 &4.223686e-04 		 &111111101100\\
a 		 &6.259503e-02 		 &0110\\
b 		 &1.022132e-02 		 &1110110\\
c 		 &1.816185e-02 		 &110101\\
d 		 &3.446528e-02 		 &10110\\
e 		 &9.072478e-02 		 &0011\\
f 		 &1.714817e-02 		 &111000\\
g 		 &1.731711e-02 		 &110110\\
h 		 &3.666160e-02 		 &10101\\
i 		 &4.443318e-02 		 &10001\\
j 		 &6.757898e-04 		 &11111101110\\
k 		 &5.321845e-03 		 &11110111\\
l 		 &3.353607e-02 		 &10111\\
m 		 &1.731711e-02 		 &110111\\
n 		 &4.967055e-02 		 &01111\\
o 		 &6.487582e-02 		 &0101\\
p 		 &1.512080e-02 		 &1110011\\
q 		 &3.378949e-04 		 &111111110011\\
r 		 &4.164555e-02 		 &10010\\
s 		 &4.130765e-02 		 &10011\\
t 		 &7.653320e-02 		 &0100\\
u 		 &2.390607e-02 		 &110000\\
v 		 &9.714479e-03 		 &1111001\\
w 		 &1.968238e-02 		 &110011\\
x 		 &1.098158e-03 		 &1111101101\\
y 		 &2.128738e-02 		 &110010\\
z 		 &3.378949e-04 		 &111111110100\\
$-$ 	 &2.534212e-04 		 &111111111000\\
\cents 	 &8.447373e-05 		 &11111111111110\\
\\ \hline
\end{tabular}
\end{minipage}
\end{table}
%----------------------------------------------------------------------------------------
%	MAJOR SECTION 2
%----------------------------------------------------------------------------------------

\subsection{Decoding for the code of document} %

\subsubsection{Decoding Huffman Code}

After coding the document \emph{Steve Jobs Speech.doc}, we must decode it to verify the correctness of our \emph{Huffman Coding}. Here we load a string from the \emph{'coding Huffman.txt'}, which is the result of \emph{Huffman Coding}, and compare the part of string, form start to next characteristic, to that $71-symbols$ codes, translate it to the corresponding symbols till the end of string, and find that we translate it to \emph{'Decoding Huffman.txt'}, which is the same as original document!

Here is our implementation of \emph{Decoding huffman code}.

\begin{lstlisting}[language=matlab]
fids = fopen('Coding_Huffman.txt','r');               % get code
[A,COUNT] = fscanf(fids,'%c');

fidss = fopen('Decoding_Huffman.txt','wt');           % Decoding Huffman code
now = 1;
next = 1;
while (next <= COUNT)
    idx = 0;
    for i = 1:num_char
        if(strcmp(A(now:next), w_H{i}) == 1)          % compare to the codewords
            idx = i;
        end
    end
    if(idx ~= 0)
        fprintf(fidss, '%s', char_doc{idx});          % translate to symbols
        now = next + 1;
        next = now + 1;
    else
        next = next + 1;                              % +1
    end
end
fclose(fids);
\end{lstlisting}
\begin{center}
  Code3: Decoding Huffman code
\end{center}


\subsubsection{Decoding Shannon Code}

The process of decoding \emph{Shannon code} is the same as decoding \emph{Huffman code}. The only difference is we must compare our part of string to the \emph{Shannon Codeword} instead of \emph{Huffman Codeword}. The results turns to be alright with our methods.

Here is our implementation of \emph{Decoding Shannon code}.
\newpage
\begin{lstlisting}[language=matlab]
fids = fopen('Coding_Shannon.txt','r');               % get code
[A,COUNT] = fscanf(fids,'%c');

fidss = fopen('Decoding_Shannon.txt','wt');           % Decoding Shannon code
now = 1;
next = 1;
while (next <= COUNT)
    idx = 0;
    for i = 1:num_char
        if(strcmp(A(now:next), w_S{i}) == 1)          % compare to the codewords
            idx = i;
        end
    end
    if(idx ~= 0)
        fprintf(fidss, '%s', char_doc{idx});          % translate to symbols
        now = next + 1;
        next = now + 1;
    else
        next = next + 1;                              % + 1
    end
end
fclose(fids);
\end{lstlisting}
\begin{center}
  Code4: Decoding Shannon code
\end{center}

\subsection{Average code length}

\subsubsection{Average codelength for Huffman coding}

The average code length for this document with \emph{Huffman Coding} is:
\begin{equation}\label{ep:eqs}
  l_{Huffman-Coding} \approx 4.428873
\end{equation}


\subsubsection{Average codelength for Shannon coding}

The average code length for this document with \emph{Shannon Coding} is:
\begin{equation}\label{ep:eqs}
  l_{Shannon-Coding} \approx 4.774286
\end{equation}

\newpage
%----------------------------------------------------------------------------------------
%	CONCLUSION
%----------------------------------------------------------------------------------------

\section{Conclusion} % Major section
From the results of two coding methods, we could find that \emph{Huffman Coding} is more efficient compared to \emph{Shannon Coding}. But neither of them can be less than or equal to the information entropy of this document, which can be seen as the replication experiment for the correctness of \emph{Shannon's First Theorem}!

Through this programming project, I get a deeper understanding of information entropy and the two coding methods: \emph{Huffman Coding} and \emph{Shannon Coding}. This programming experiment can promote my learning of \emph{Information Theorem Course} and the ability for implementing algorithm. Thanks to this programming project, I could implement coding process by myself without extra help except the algorithm provided by teacher.

\vspace{1\baselineskip}

\emph{Wish I could go further in the information road!}

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------
\newpage
\begin{thebibliography}{99} % Bibliography - this is intentionally simple in this template

\bibitem{1}
Thomas M.Cover ,  Joy A.Thomas .
\newblock  Elements of Information Theory.
\newblock  Wiley-Blackwell Press.
\end{thebibliography}
%----------------------------------------------------------------------------------------



\end{document} 